{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c50dfd98-32a8-4f38-8496-041ed58e2609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: cannot set terminal process group (33925): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from subprocess import check_output\n",
    "\n",
    "#to ensure jupyter \"sees\" OpenAI API key in bashrc\n",
    "def load_bashrc():\n",
    "    bashrc_contents = check_output(['bash', '-i', '-c', 'echo $OPENAI_API_KEY'])\n",
    "    os.environ['OPENAI_API_KEY'] = bashrc_contents.decode('utf-8').strip()\n",
    "\n",
    "load_bashrc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b52fbda-c50d-4142-8fb0-c21ec96da80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from paperqa import Settings, Docs\n",
    "\n",
    "async def rag_agent(question, vector_store, rag_model) -> str:\n",
    "    \"\"\"\n",
    "    Runs the PaperQA2 RAG, returning the answer to the inputted question, given the vector_store object.\n",
    "    \n",
    "    Args:\n",
    "        question: Question to be answered\n",
    "        vector_store: Documents to be searched for answers (object class is part of PaperQA2 library)\n",
    "        rag_model: LLM that can be used to power PaperQA2 RAG\n",
    "    Returns:\n",
    "        Answer to the inputted question\n",
    "    \"\"\"\n",
    "    settings.temperature=0.0\n",
    "    settings.llm = rag_model\n",
    "\n",
    "    answer_response = await vector_store.aquery(\n",
    "        query=question,\n",
    "        settings=settings   \n",
    "    )\n",
    "    response = answer_response.model_dump()\n",
    "    return response[\"answer\"]\n",
    "\n",
    "#\"directory\" is directory in which papers used in RAG can be found\n",
    "directory = \"/home/adrian/Documents/University Work/Part III Project/cmbagent_dataset/Source_Papers\"\n",
    "\n",
    "settings = Settings(\n",
    "    parsing={\n",
    "        \"use_doc_details\": False,  # Disable metadata extraction\n",
    "        \"disable_doc_valid_check\": True  # Skip document validation\n",
    "    },\n",
    ")\n",
    "\n",
    "vector_store = Docs()\n",
    "# valid extensions include .pdf, .txt, .md, and .html\n",
    "full_paths = [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory)]\n",
    "\n",
    "for doc in (full_paths):\n",
    "    #use async add PaperQA2 function in Jupyter notebook \n",
    "    await vector_store.aadd(doc, settings=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "facb6c7d-6f09-4eea-a20c-57b1287f3a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/adrian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/adrian/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nDemonstration of how eval_agent and embedding_answers are used to perform the Embed_AI performance evaluation algorithm\\n\\neval_ai = await eval_agent(this_question, this_answer, this_ideal, eval_model)\\nembedding_eval = await embedding_answers(this_answer, this_ideal, custom_stopwords, english_words)\\n\\nif (embedding_eval >= 0.8 and (eval_ai in [\"Same\", \"Similar\"])):\\n            #Embed_AI evaluation algorithm will consider a generated answer \"correct\" if both the embedding_eval score is >=0.8 and if the AI evaluation returns \"Same\" or \"Similar\".\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import re\n",
    "import time\n",
    "from rake_nltk import Rake\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "client = OpenAI(api_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "#code to set up keyphrase extraction + extraction of questions and ideal from files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "\n",
    "custom_stopwords = set(nltk.corpus.stopwords.words('english')) - {\"no\", \"not\", \"than\", \"more\", \"same\", \"before\", \"after\", \"now\", \"then\", \"above\", \"below\", \"over\", \"under\", \"like\", \"other\", \"such\", \"few\", \"most\", \"some\", \"between\"}  # Keep logical comparatives- important for RAG analysis \n",
    "\n",
    "class eval_format(BaseModel):\n",
    "    Evaluation: Literal[\"Same\", \"Similar\", \"Different\"] = Field(\n",
    "    description=r\"\"\"If a point is conveyed in both answers, as responses to the associated question, output \"Same\".\n",
    "    If a similar points is conveyed in both answers, as responses to the associated question, output \"Similar\".\n",
    "    If all of the points are different in both answers, as responses to the associated question, output \"Different\".\"\"\")\n",
    "\n",
    "#function for the evaluation agent\n",
    "async def eval_agent(question, answer, ideal, eval_model) -> str:\n",
    "    \"\"\"\n",
    "    Runs the OpenAI Evaluation AI\n",
    "    \n",
    "    Args:\n",
    "        question: Question that the two answers are answering (included for context)\n",
    "        answer: Generated answer to the question\n",
    "        ideal: \"Ideal\" answer the generated answer is to be compared to.\n",
    "        eval_model: OpenAI model to power the agent\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation in the form of \"Same\", \"Similar\" or \"Different\". If the API call fails, returns \"N/A\"\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_message=\"\"\"\n",
    "    You are an evaluation agent tasked with comparing the given two different answers to the same question. \n",
    "    Focus on the meaning of both answers, in the context of the question, when formulating your evaluation.\n",
    "    If you are unsure about the above criteria for the answers to the associated question, output \"Unsure\".\n",
    "    Ensure that differences between numerical values and results between the two answers are emphasised in your analysis, unless the question specifically allows for approximations/inexact numerical values. \n",
    "    Then, if the question specifically allows for approximations/inexact numerical values, only compare the numerical values approximately.\n",
    "    \"\"\"\n",
    "    eval_assistant = client.beta.assistants.create(\n",
    "        name=\"eval_test\",\n",
    "        instructions=eval_message,\n",
    "        model=eval_model, \n",
    "        temperature = 0.0,\n",
    "        top_p = 0.2,\n",
    "        response_format= {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"answer\",\n",
    "                \"schema\": eval_format.model_json_schema()\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    thread = client.beta.threads.create(\n",
    "                    messages=[],\n",
    "                )\n",
    "    \n",
    "    parsed = client.beta.threads.messages.create(\n",
    "                    thread_id=thread.id,\n",
    "                    content=question+answer+str(ideal),\n",
    "                    role='user',\n",
    "                )\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=eval_assistant.id,\n",
    "        # pass the latest system message as instructions\n",
    "        instructions=eval_message,\n",
    "    )\n",
    "    run = client.beta.threads.runs.retrieve(run.id, thread_id=thread.id)\n",
    "    while run.status==\"queued\" or run.status==\"in_progress\":\n",
    "        time.sleep(0.1)\n",
    "        run = client.beta.threads.runs.retrieve(run.id, thread_id=thread.id)\n",
    "    if run.status==\"completed\":\n",
    "        response_messages = client.beta.threads.messages.list(thread.id, order=\"asc\")\n",
    "        for message in response_messages.data:\n",
    "            for content in message.content:\n",
    "                output=content.text.value\n",
    "                if output.startswith(\"{\"):\n",
    "                    data=json.loads(output)\n",
    "                    try:\n",
    "                        evaluation=data.get(\"Evaluation\")\n",
    "                    except:\n",
    "                        print(\"Evaluation not found\", end=\"\\r\", flush=True)\n",
    "    if not (\"evaluation\" in locals()):\n",
    "        evaluation=\"N/A\"\n",
    "    try:\n",
    "        client.beta.assistants.delete(assistant_id=eval_assistant.id)\n",
    "    except:\n",
    "        pass\n",
    "    return evaluation\n",
    "    \n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text for keyphrase extraction\n",
    "    \"\"\"\n",
    "    # Replace decimals/commas in numbers with an underscore and replace hyphens with underscores, generally (except for negative numbers).\n",
    "    #It is only these cases that the sentence tokenizer in Rake doesn't seem to handle well\n",
    "    text = re.sub(r'(\\d+)\\.(\\d+)', r'\\1_\\2', text)\n",
    "    text = re.sub(r'(\\d+)\\,(\\d+)', r'\\1\\2', text)\n",
    "    #cursive l in text is formatted strangely in ChatGPT output\n",
    "    text = text.replace(\"`\", \"l\")\n",
    "    # Pattern explanation:\n",
    "    # (?<!\\s)-(?!\\d) - matches hyphens not preceded by whitespace or followed by digit\n",
    "    # | - OR\n",
    "    # (?<=\\s)-(?=\\D) - matches hyphens preceded by whitespace and followed by non-digit\n",
    "    text = re.sub(r'(?<!\\s)-(?!\\d)|(?<=\\s)-(?=\\D)', '_', text)\n",
    "    return text\n",
    "    \n",
    "#function for the enbedding answers algorithm\n",
    "async def embedding_answers(answer, ideal, custom_stopwords, english_words) -> str:\n",
    "    \"\"\"\n",
    "    Novel part of AI evaluation algorithm. This algorithm extracts the keyphrases from the generated and \"ideal\" answers and then compares the cosine similarity of the vector embeddings between the keyphrases of the \"ideal\" answer and the generated answer. It gets the maximum cosine similarity for each keyphrase in the \"ideal\" answer and takes the mean of all of them. This mean is the returned \"score\". There is some additional preprocessing due to formatting and additional handling of \"names\" that may not have a meaningful vector embedding, but that is the main idea.\n",
    "    \n",
    "    Args:\n",
    "        answer: Generated answer to the question\n",
    "        ideal: \"Ideal\" answer the generated answer is to be compared to.\n",
    "        custom_stopwords: A list of common words for the keyphrase extractor to automatically ignore.\n",
    "        english_words: A list of words in english\n",
    "    Returns:\n",
    "        A mean score between 0 and 1 (in practise, between ~0.7 and 1). Generated answer considered \"correct\" if mean score >=0.8 \n",
    "    \"\"\"\n",
    "    #tell Rake to leave logical comparatives alone\n",
    "    r = Rake(stopwords=custom_stopwords)\n",
    "    #Extraction given the text.\n",
    "    text1=preprocess_text(answer)\n",
    "    #ideal is formatted using latex for CosmoPaperQA\n",
    "    ideal=LatexNodes2Text().latex_to_text(ideal)\n",
    "    text2=preprocess_text(ideal)\n",
    "    r.extract_keywords_from_text(text1)\n",
    "    key_phrases1=r.get_ranked_phrases()\n",
    "    r.extract_keywords_from_text(text2)\n",
    "    key_phrases2=r.get_ranked_phrases()\n",
    "    result_1=[]\n",
    "    for string_ideal in key_phrases2:\n",
    "        #check for \"names\" that need to be matched exactly\n",
    "        #checks that string_ideal is one word with at least one letter and that is not in english\n",
    "        if (not (\" \" in string_ideal)) and (any(char.isalpha() for char in string_ideal)) and (not (string_ideal in english_words)):\n",
    "            #if this word does exist in the answer...\n",
    "            string_ideal=string_ideal.replace(\"_\", \"\")\n",
    "            #sort out odd formatting issues surrounding underscores in \"names\"\n",
    "            if (string_ideal in text1.lower()):\n",
    "                #we have a match!\n",
    "                result_1.append(1)\n",
    "            else:\n",
    "                #if not, no match, therefore \"incorrect\"\n",
    "                #0.7 works welll as \"incorrect\" cosine similarity for text-embedding-ada-002 model. \n",
    "                #If using text-embedding-3-large model, 0.3 works better as \"incorrect\" cosine similarity\n",
    "                result_1.append(0.7)\n",
    "        else:\n",
    "            max_cos=0\n",
    "            check=0\n",
    "            for string_gen in key_phrases1:\n",
    "                if (string_ideal==string_gen and check==0):\n",
    "                    max_cos=1\n",
    "                    result_1.append(max_cos)\n",
    "                    check=1\n",
    "            if (max_cos!=1):\n",
    "                resp1 = client.embeddings.create(\n",
    "                    input=string_ideal,\n",
    "                    model=\"text-embedding-ada-002\",\n",
    "                    encoding_format= \"float\",\n",
    "                )\n",
    "                for string_gen in key_phrases1:\n",
    "                    resp2 = client.embeddings.create(\n",
    "                        input=string_gen,\n",
    "                        model=\"text-embedding-ada-002\",\n",
    "                        encoding_format= \"float\",\n",
    "                    )\n",
    "                    a=np.array(resp1.data[0].embedding)\n",
    "                    b=np.array(resp2.data[0].embedding)\n",
    "                    cos=np.dot(a,b)/(norm(a)*norm(b))\n",
    "                    if (cos>max_cos):\n",
    "                        max_cos=cos\n",
    "                result_1.append(max_cos)\n",
    "    #mean is a crude way to combine these scores.\n",
    "    #will consider \"correct\" if mean >=0.8, otherwise \"incorrect\" (also a crude metric)\n",
    "    #0.8 value is designed for text-embedding-ada-002 model. If using text-embedding-3-large model, 0.4 works better\n",
    "    return np.mean(np.array(result_1)) \n",
    "\n",
    "\"\"\"\n",
    "Demonstration of how eval_agent and embedding_answers are used to perform the Embed_AI performance evaluation algorithm\n",
    "\n",
    "eval_ai = await eval_agent(this_question, this_answer, this_ideal, eval_model)\n",
    "embedding_eval = await embedding_answers(this_answer, this_ideal, custom_stopwords, english_words)\n",
    "\n",
    "if (embedding_eval >= 0.8 and (eval_ai in [\"Same\", \"Similar\"])):\n",
    "            #Embed_AI evaluation algorithm will consider a generated answer \"correct\" if both the embedding_eval score is >=0.8 and if the AI evaluation returns \"Same\" or \"Similar\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f182dc37-8b99-4d82-8dbf-de39c8c20520",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=[]\n",
    "ideal=[]\n",
    "\n",
    "#input single question here\n",
    "question.append(r'In the \"Cosmology with one galaxy?\" paper, how is the direct comparison of the performance of the IllustrisTNG and the SIMBA simulations performed?')\n",
    "ideal.append(r\"There is no correspondence between simulations among the IllustrisTNG and the SIMBA sets. Thus, a direct comparison between the two is not performed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bcf8c20-5677-4874-8219-c038a0cfc7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydantic import BaseModel, Field, conlist\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import re\n",
    "from rake_nltk import Rake\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "import nltk\n",
    "import json\n",
    "from typing import Literal\n",
    "from typing import Any\n",
    "\n",
    "from inspect_ai.solver import (\n",
    "    TaskState,\n",
    "    solver,\n",
    ")\n",
    "from inspect_ai import Task, task\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.scorer import (\n",
    "    CORRECT,\n",
    "    INCORRECT,\n",
    "    Score,\n",
    "    Target,\n",
    "    accuracy,\n",
    "    stderr,\n",
    "    scorer,\n",
    ")\n",
    "from inspect_ai.solver import bridge\n",
    "from inspect_ai import eval\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#poor man's pass by ref\n",
    "class CSVHolder:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "#file path to the csv file containing the evaluation dataset\n",
    "lit = pd.read_csv('../cmbagent_dataset/cmbagent_dataset.csv', delimiter=\"\\t\")\n",
    "\n",
    "def inspect_ai_eval(rag_agent, eval_agent, embedding_answers):\n",
    "    \"\"\"\n",
    "    Function to implement inspect_ai evaluation using rag_agent and eval_agent functions\n",
    "\n",
    "    Args:\n",
    "        rag_agent: function that implements the RAG for the generated answers to the questions in the dataset\n",
    "        eval_agent: function that implements the AI evaluation agent for Embed_AI\n",
    "        embedding_answers: function that implements the cosine similarity of vector embeddings component of Embed_AI\n",
    "    Returns:\n",
    "        nominally None, but will print out a csv file with all of the relevant evaluation and RAG information present.\n",
    "    \"\"\"\n",
    "    #setup mytasks for evaluation\n",
    "    mytasks = []\n",
    "    for i in range(len(question)):\n",
    "        mytasks.append({\n",
    "            \"input\": question[i],\n",
    "            \"target\": ideal[i]\n",
    "        })\n",
    "    #setup output DataFrame\n",
    "    new_output_holder= CSVHolder(pd.DataFrame({\n",
    "        'question': pd.Series(dtype='object'),\n",
    "        'answer': pd.Series(dtype='object'),\n",
    "        'ideal': pd.Series(dtype='object'),\n",
    "        'AI_eval': pd.Series(dtype='object'),\n",
    "        'embedding_eval': pd.Series(dtype='float'),\n",
    "        'evaluation': pd.Series(dtype='object')\n",
    "    }))\n",
    "    \n",
    "    async def my_agent(task_input: list[dict[str, Any]]) -> str:\n",
    "        #replace rag_agent function if needed, to implement custom RAG agent.\n",
    "        #Can put OpenAI RAG agent or PaperQA2 RAG agent here\n",
    "        answer = await rag_agent(task_input[0][\"content\"], vector_store, \"gpt-4o-mini\")\n",
    "        return answer\n",
    "    \n",
    "    @solver\n",
    "    def my_solver():\n",
    "        async def solve(state: TaskState) -> TaskState:\n",
    "            result = await my_agent(state[\"input\"])\n",
    "            return {\"output\":result}\n",
    "        return solve\n",
    "    \n",
    "    @task\n",
    "    def my_task(tasks):\n",
    "        return Task(\n",
    "            dataset=[Sample(\n",
    "                input=tasks[i][\"input\"],\n",
    "                target=tasks[i][\"target\"]\n",
    "            ) for i in range(len(tasks))],\n",
    "            solver = bridge(my_solver()),\n",
    "            #replace \"gpt-4o-mini\" with model you want to use for evaluation AI\n",
    "            scorer = my_scorer(\"gpt-4o-mini\", custom_stopwords, english_words, new_output_holder),\n",
    "        )\n",
    "    \n",
    "    @scorer(metrics=[accuracy(), stderr()])\n",
    "    def my_scorer(eval_model: str, custom_stopwords: set, english_words: set, new_output_holder):\n",
    "        async def score(state: TaskState, target: Target) -> Score:\n",
    "            this_question = state.input_text\n",
    "            this_answer = state.output.completion\n",
    "            this_ideal = target.text\n",
    "\n",
    "            #replace eval_agent function if needed, to implement custom evaluation agent\n",
    "            eval_ai = await eval_agent(this_question, this_answer, this_ideal, eval_model)\n",
    "            embedding_eval = await embedding_answers(this_answer, this_ideal, custom_stopwords, english_words)\n",
    "    \n",
    "            #sort out new_output pass by ref\n",
    "            if (embedding_eval >= 0.8 and (eval_ai in [\"Same\", \"Similar\"])):\n",
    "                #Embed_AI evaluation algorithm will consider a generated answer \"correct\" if both the embedding_eval score is >=0.8 and if the AI evaluation returns \"Same\" or \"Similar\".\n",
    "                new_entry=pd.DataFrame({\"question\":this_question, \"answer\":this_answer, \"ideal\":this_ideal, \"AI_eval\": eval_ai, \"embedding_eval\": embedding_eval, \"evaluation\":\"CORRECT\"}, index=[0])\n",
    "                new_output_holder.value=pd.concat([new_output_holder.value, new_entry], ignore_index=True)\n",
    "                new_output_holder.value.to_csv(\"output_Single_Question_Example.csv\", index=False)\n",
    "                return Score(value=CORRECT)\n",
    "            else:\n",
    "                new_entry=pd.DataFrame({\"question\":this_question, \"answer\":this_answer, \"ideal\":this_ideal, \"AI_eval\": eval_ai, \"embedding_eval\": embedding_eval, \"evaluation\":\"INCORRECT\"}, index=[0])\n",
    "                new_output_holder.value=pd.concat([new_output_holder.value, new_entry], ignore_index=True)\n",
    "                new_output_holder.value.to_csv(\"output_Single_Question_Example.csv\", index=False)\n",
    "                return Score(value=INCORRECT)\n",
    "        return score\n",
    "    \n",
    "    new_output_holder.value.to_csv(\"output_Single_Question_Example.csv\", index=False)\n",
    "    logs = eval(\n",
    "        my_task(mytasks)\n",
    "    )\n",
    "    print(logs)\n",
    "    for log in logs:\n",
    "        print(log.results)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0907fc34-6376-400a-9175-514be84e8f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">my_task (1 sample): none/none</span> ─────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ <span style=\"color: #808080; text-decoration-color: #808080\">tasks: {'input': 'In the \"Cosmology with one galaxy?\" paper, how is the direct comparison of </span><span style=\"color: #000080; text-decoration-color: #000080\">dataset: (samples)</span> │\n",
       "│ <span style=\"color: #808080; text-decoration-color: #808080\">the performance of the IllustrisTNG and the SIMBA simulations performed?', 'target': 'There </span>                    │\n",
       "│ <span style=\"color: #808080; text-decoration-color: #808080\">is no correspondence between simulations among the IllustrisTNG and the SIMBA sets. Thus, a </span>                    │\n",
       "│ <span style=\"color: #808080; text-decoration-color: #808080\">direct comparison between the two is not performed.'}</span>                                                           │\n",
       "│                                                                                                                 │\n",
       "│ <span style=\"color: #000080; text-decoration-color: #000080\">⠿</span> my_task none/none                                                             <span style=\"color: #800080; text-decoration-color: #800080\">  0%</span> 0/1 accuracy:  n/a <span style=\"color: #808000; text-decoration-color: #808000\">0:00:11</span> │\n",
       "│                                                                                                                 │\n",
       "│ <span style=\"color: #808080; text-decoration-color: #808080\">                                                                                                </span><span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">HTTP retries:</span><span style=\"color: #808080; text-decoration-color: #808080\"> 0</span> │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ \u001b[1mmy_task (1 sample): none/none\u001b[0m ─────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ tasks: {'input': 'In the \"Cosmology with one galaxy?\" paper, how is the direct comparison of dataset: (samples) │\n",
       "│ the performance of the IllustrisTNG and the SIMBA simulations performed?', 'target': 'There                     │\n",
       "│ is no correspondence between simulations among the IllustrisTNG and the SIMBA sets. Thus, a                     │\n",
       "│ direct comparison between the two is not performed.'}                                                           │\n",
       "│                                                                                                                 │\n",
       "│ ⠿ my_task none/none                                                               0% 0/1 accuracy:  n/a 0:00:11 │\n",
       "│                                                                                                                 │\n",
       "│                                                                                                 \u001b[1mHTTP retries:\u001b[0m 0 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inspect_ai_eval(rag_agent, eval_agent, embedding_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmbagent_env",
   "language": "python",
   "name": "cmbagent_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
