{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169e73d-8d69-4e77-a437-165a77fe40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "client = OpenAI(api_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "#Vector store for files used for RAG, change this to whatever vector store you have for OpenAI. \n",
    "#See Create_Vector_Store_Example.ipynb in the repository to see how to do this.\n",
    "vector_store=client.vector_stores.retrieve(vector_store_id=\"vs_67da9f09a6b48191a32189befe73c49e\")\n",
    "\n",
    "class rag_format(BaseModel):\n",
    "    Ranked_Relevant_Information: str = Field(description=\"The ranked pieces of information that will be directly relevant for answering the query.\")\n",
    "    File_Sources: str = Field(description=\"The filenames of the files from which the information was retrieved, with the format '{...}.pdf'.\")\n",
    "\n",
    "class answer_format(BaseModel):\n",
    "    Response: str = Field(description=\"The answer to the question/prompt using the given information only.\")\n",
    "\n",
    "\n",
    "def rag_agent(question, vector_store, rag_model) -> str:\n",
    "    \"\"\"\n",
    "    Runs the OpenAI RAG, returning the answer to the inputted question, given the documents in the vector_store.\n",
    "    \n",
    "    Args:\n",
    "        question: Question to be answered\n",
    "        vector_store: OpenAI vector store containing the documents used to search for answers\n",
    "        rag_model: LLM that can be used to power OpenAI RAG\n",
    "    Returns:\n",
    "        Answer to the inputted question\n",
    "    \"\"\"\n",
    "    \n",
    "    rag_message=\"\"\"You are a retrieval agent tasked with performing file searches to find information for the purpose of providing answers.\n",
    "        Find pieces of information that will be directly relevant for answering the query and rank these pieces of information from most relevant to least relevant\n",
    "        You must quote the passages from the files directly. Do not paraphrase or change the text in any way.\n",
    "        Do not include information unless you have a source for that piece of information. \n",
    "        If no information is relevant, you must return a single piece of information, where you state \"No information found\".\n",
    "        Ideally, these pieces of information will be sentences, phrases, data points or sets of data points, but you have limited flexibility to include other pieces of information if you think they are appropriate.\n",
    "        \n",
    "        You must use tool call (i.e., file search).\n",
    "        \n",
    "        You know about the content of the code-base.\n",
    "        \"\"\"\n",
    "    rag_assistant = client.beta.assistants.create(\n",
    "        name=\"rag_test\",\n",
    "        instructions=rag_message,\n",
    "        tools=[\n",
    "            {\"type\": \"file_search\",\n",
    "                \"file_search\":{\n",
    "                    'max_num_results': 10,\n",
    "                    \"ranking_options\": {\n",
    "                        \"ranker\": \"auto\",\n",
    "                        \"score_threshold\": 0.6\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        tool_resources={\"file_search\": {\"vector_store_ids\":[vector_store.id]}},\n",
    "        model=rag_model, \n",
    "        temperature = 0,\n",
    "        top_p = 0.2,\n",
    "        response_format= {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"answer\",\n",
    "                \"schema\": rag_format.model_json_schema()\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    thread = client.beta.threads.create(\n",
    "                    messages=[],\n",
    "                )\n",
    "    \n",
    "    parsed = client.beta.threads.messages.create(\n",
    "                    thread_id=thread.id,\n",
    "                    content=question,\n",
    "                    role='user',\n",
    "                )\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=rag_assistant.id,\n",
    "        # pass the latest system message as instructions\n",
    "        instructions=rag_message,\n",
    "        tool_choice={\"type\": \"file_search\", \"function\": {\"name\": \"file_search\"}}\n",
    "    )\n",
    "    run = client.beta.threads.runs.retrieve(run.id, thread_id=thread.id)\n",
    "    while run.status==\"queued\" or run.status==\"in_progress\":\n",
    "        time.sleep(0.1)\n",
    "        run = client.beta.threads.runs.retrieve(run.id, thread_id=thread.id)\n",
    "    if run.status==\"completed\":\n",
    "        response_messages = client.beta.threads.messages.list(thread.id, order=\"asc\")\n",
    "        for message in response_messages.data:\n",
    "            for content in message.content:\n",
    "                output=content.text.value\n",
    "                if output.startswith(\"{\"):\n",
    "                    data=json.loads(output)\n",
    "                    try:\n",
    "                        answer = data.get(\"Ranked_Relevant_Information\") or data.get(\"Ranked Relevant Information\")\n",
    "                        sources = data.get(\"File_Sources\") or data.get(\"File Sources\")\n",
    "                    except:\n",
    "                        print(\"Ranked_Relevant_Information/File_Sources not found\", end=\"\\r\", flush=True)\n",
    "    if not (\"answer\" in locals()):\n",
    "        answer=\"No relevant information.\"\n",
    "    if not (\"sources\" in locals()):\n",
    "        sources=\"No relevant sources.\"\n",
    "    client.beta.assistants.delete(assistant_id=rag_assistant.id)\n",
    "    answer_message=\"\"\"\n",
    "    You are an answering agent tasked with answering a question or providing a summary only using the relevant information or prompts that are given to you, via the \"Ranked Relevant Information\".\n",
    "    Generate a logical and reasoned response to the question or prompts only using the ranked relevant information.\n",
    "    Use the question to provide context to the information before deciding if the information is relevant or not.\n",
    "    If no file sources are given, you must answer \"No information.\".\n",
    "    \"\"\"\n",
    "    answer_assistant = client.beta.assistants.create(\n",
    "        name=\"answer_test\",\n",
    "        instructions=answer_message,\n",
    "        model=rag_model, \n",
    "        temperature = 0.0,\n",
    "        top_p = 0.2,\n",
    "        response_format= {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"answer\",\n",
    "                \"schema\": answer_format.model_json_schema()\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    thread = client.beta.threads.create(\n",
    "                    messages=[],\n",
    "                )\n",
    "    \n",
    "    parsed = client.beta.threads.messages.create(\n",
    "                    thread_id=thread.id,\n",
    "                    content=\"Question: \"+question+\"\\nRanked Relevant Information: \"+answer,\n",
    "                    role='user',\n",
    "                )\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=answer_assistant.id,\n",
    "        # pass the latest system message as instructions\n",
    "        instructions=answer_message,\n",
    "    )\n",
    "    del answer\n",
    "    run = client.beta.threads.runs.retrieve(run.id, thread_id=thread.id)\n",
    "    while run.status==\"queued\" or run.status==\"in_progress\":\n",
    "        time.sleep(0.1)\n",
    "        run = client.beta.threads.runs.retrieve(run.id, thread_id=thread.id)\n",
    "    if run.status==\"completed\":\n",
    "        response_messages = client.beta.threads.messages.list(thread.id, order=\"asc\")\n",
    "        for message in response_messages.data:\n",
    "            for content in message.content:\n",
    "                output=content.text.value\n",
    "                if output.startswith(\"{\"):\n",
    "                    data=json.loads(output)\n",
    "                    try:\n",
    "                        answer=data.get(\"Response\")\n",
    "                    except:\n",
    "                        print(\"Response not found\", end=\"\\r\", flush=True)\n",
    "    if not (\"answer\" in locals()):\n",
    "        answer=\"No information.\"\n",
    "    client.beta.assistants.delete(assistant_id=answer_assistant.id)\n",
    "    return answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmbagent_env",
   "language": "python",
   "name": "cmbagent_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
