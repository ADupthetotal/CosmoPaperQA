{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169e73d-8d69-4e77-a437-165a77fe40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import re\n",
    "import time\n",
    "from rake_nltk import Rake\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "client = OpenAI(api_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "#code to set up keyphrase extraction + extraction of questions and ideal from files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "\n",
    "custom_stopwords = set(nltk.corpus.stopwords.words('english')) - {\"no\", \"not\", \"than\", \"more\", \"same\", \"before\", \"after\", \"now\", \"then\", \"above\", \"below\", \"over\", \"under\", \"like\", \"other\", \"such\", \"few\", \"most\", \"some\", \"between\"}  # Keep logical comparatives- important for RAG analysis \n",
    "\n",
    "class eval_format(BaseModel):\n",
    "    Evaluation: Literal[\"Same\", \"Similar\", \"Different\"] = Field(\n",
    "    description=r\"\"\"If a point is conveyed in both answers, as responses to the associated question, output \"Same\".\n",
    "    If a similar points is conveyed in both answers, as responses to the associated question, output \"Similar\".\n",
    "    If all of the points are different in both answers, as responses to the associated question, output \"Different\".\"\"\")\n",
    "\n",
    "#function for the evaluation agent\n",
    "async def eval_agent(question, answer, ideal, eval_model) -> str:\n",
    "    \"\"\"\n",
    "    Runs the OpenAI Evaluation AI\n",
    "    \n",
    "    Args:\n",
    "        question: Question that the two answers are answering (included for context)\n",
    "        answer: Generated answer to the question\n",
    "        ideal: \"Ideal\" answer the generated answer is to be compared to.\n",
    "        eval_model: OpenAI model to power the agent\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation in the form of \"Same\", \"Similar\" or \"Different\". If the API call fails, returns \"N/A\"\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_message=\"\"\"\n",
    "    You are an evaluation agent tasked with comparing the given two different answers to the same question. \n",
    "    Focus on the meaning of both answers, in the context of the question, when formulating your evaluation.\n",
    "    If you are unsure about the above criteria for the answers to the associated question, output \"Unsure\".\n",
    "    Ensure that differences between numerical values and results between the two answers are emphasised in your analysis, unless the question specifically allows for approximations/inexact numerical values. \n",
    "    Then, if the question specifically allows for approximations/inexact numerical values, only compare the numerical values approximately.\n",
    "    \"\"\"\n",
    "    eval_assistant = client.beta.assistants.create(\n",
    "        name=\"eval_test\",\n",
    "        instructions=eval_message,\n",
    "        model=eval_model, \n",
    "        temperature = 0.0,\n",
    "        top_p = 0.2,\n",
    "        response_format= {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"answer\",\n",
    "                \"schema\": eval_format.model_json_schema()\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    thread = client.beta.threads.create(\n",
    "                    messages=[],\n",
    "                )\n",
    "    \n",
    "    parsed = client.beta.threads.messages.create(\n",
    "                    thread_id=thread.id,\n",
    "                    content=question+answer+str(ideal),\n",
    "                    role='user',\n",
    "                )\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=eval_assistant.id,\n",
    "        # pass the latest system message as instructions\n",
    "        instructions=eval_message,\n",
    "    )\n",
    "    run = client.beta.threads.runs.retrieve(run.id, thread_id=thread.id)\n",
    "    while run.status==\"queued\" or run.status==\"in_progress\":\n",
    "        time.sleep(0.1)\n",
    "        run = client.beta.threads.runs.retrieve(run.id, thread_id=thread.id)\n",
    "    if run.status==\"completed\":\n",
    "        response_messages = client.beta.threads.messages.list(thread.id, order=\"asc\")\n",
    "        for message in response_messages.data:\n",
    "            for content in message.content:\n",
    "                output=content.text.value\n",
    "                if output.startswith(\"{\"):\n",
    "                    data=json.loads(output)\n",
    "                    try:\n",
    "                        evaluation=data.get(\"Evaluation\")\n",
    "                    except:\n",
    "                        print(\"Evaluation not found\", end=\"\\r\", flush=True)\n",
    "    if not (\"evaluation\" in locals()):\n",
    "        evaluation=\"N/A\"\n",
    "    client.beta.assistants.delete(assistant_id=eval_assistant.id)\n",
    "    return evaluation\n",
    "    \n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text for keyphrase extraction\n",
    "    \"\"\"\n",
    "    # Replace decimals/commas in numbers with an underscore and replace hyphens with underscores, generally (except for negative numbers).\n",
    "    #It is only these cases that the sentence tokenizer in Rake doesn't seem to handle well\n",
    "    text = re.sub(r'(\\d+)\\.(\\d+)', r'\\1_\\2', text)\n",
    "    text = re.sub(r'(\\d+)\\,(\\d+)', r'\\1\\2', text)\n",
    "    #cursive l in text is formatted strangely in ChatGPT output\n",
    "    text = text.replace(\"`\", \"l\")\n",
    "    # Pattern explanation:\n",
    "    # (?<!\\s)-(?!\\d) - matches hyphens not preceded by whitespace or followed by digit\n",
    "    # | - OR\n",
    "    # (?<=\\s)-(?=\\D) - matches hyphens preceded by whitespace and followed by non-digit\n",
    "    text = re.sub(r'(?<!\\s)-(?!\\d)|(?<=\\s)-(?=\\D)', '_', text)\n",
    "    return text\n",
    "    \n",
    "#function for the enbedding answers algorithm\n",
    "async def embedding_answers(answer, ideal, custom_stopwords, english_words) -> str:\n",
    "    \"\"\"\n",
    "    Novel part of AI evaluation algorithm. This algorithm extracts the keyphrases from the generated and \"ideal\" answers and then compares the cosine similarity of the vector embeddings between the keyphrases of the \"ideal\" answer and the generated answer. It gets the maximum cosine similarity for each keyphrase in the \"ideal\" answer and takes the mean of all of them. This mean is the returned \"score\". There is some additional preprocessing due to formatting and additional handling of \"names\" that may not have a meaningful vector embedding, but that is the main idea.\n",
    "    \n",
    "    Args:\n",
    "        answer: Generated answer to the question\n",
    "        ideal: \"Ideal\" answer the generated answer is to be compared to.\n",
    "        custom_stopwords: A list of common words for the keyphrase extractor to automatically ignore.\n",
    "        english_words: A list of words in english\n",
    "    Returns:\n",
    "        A mean score between 0 and 1 (in practise, between ~0.7 and 1). Generated answer considered \"correct\" if mean score >=0.8 \n",
    "    \"\"\"\"\n",
    "    #tell Rake to leave logical comparatives alone\n",
    "    r = Rake(stopwords=custom_stopwords)\n",
    "    #Extraction given the text.\n",
    "    text1=preprocess_text(answer)\n",
    "    #ideal is formatted using latex for CosmoPaperQA\n",
    "    ideal=LatexNodes2Text().latex_to_text(ideal)\n",
    "    text2=preprocess_text(ideal)\n",
    "    r.extract_keywords_from_text(text1)\n",
    "    key_phrases1=r.get_ranked_phrases()\n",
    "    r.extract_keywords_from_text(text2)\n",
    "    key_phrases2=r.get_ranked_phrases()\n",
    "    result_1=[]\n",
    "    for string_ideal in key_phrases2:\n",
    "        #check for \"names\" that need to be matched exactly\n",
    "        #checks that string_ideal is one word with at least one letter and that is not in english\n",
    "        if (not (\" \" in string_ideal)) and (any(char.isalpha() for char in string_ideal)) and (not (string_ideal in english_words)):\n",
    "            #if this word does exist in the answer...\n",
    "            string_ideal=string_ideal.replace(\"_\", \"\")\n",
    "            #sort out odd formatting issues surrounding underscores in \"names\"\n",
    "            if (string_ideal in text1.lower()):\n",
    "                #we have a match!\n",
    "                result_1.append(1)\n",
    "            else:\n",
    "                #if not, no match, therefore \"incorrect\"\n",
    "                #0.7 works welll as \"incorrect\" cosine similarity for text-embedding-ada-002 model. \n",
    "                #If using text-embedding-3-large model, 0.3 works better as \"incorrect\" cosine similarity\n",
    "                result_1.append(0.7)\n",
    "        else:\n",
    "            max_cos=0\n",
    "            check=0\n",
    "            for string_gen in key_phrases1:\n",
    "                if (string_ideal==string_gen and check==0):\n",
    "                    max_cos=1\n",
    "                    result_1.append(max_cos)\n",
    "                    check=1\n",
    "            if (max_cos!=1):\n",
    "                resp1 = client.embeddings.create(\n",
    "                    input=string_ideal,\n",
    "                    model=\"text-embedding-ada-002\",\n",
    "                    encoding_format= \"float\",\n",
    "                )\n",
    "                for string_gen in key_phrases1:\n",
    "                    resp2 = client.embeddings.create(\n",
    "                        input=string_gen,\n",
    "                        model=\"text-embedding-ada-002\",\n",
    "                        encoding_format= \"float\",\n",
    "                    )\n",
    "                    a=np.array(resp1.data[0].embedding)\n",
    "                    b=np.array(resp2.data[0].embedding)\n",
    "                    cos=np.dot(a,b)/(norm(a)*norm(b))\n",
    "                    if (cos>max_cos):\n",
    "                        max_cos=cos\n",
    "                result_1.append(max_cos)\n",
    "    #mean is a crude way to combine these scores.\n",
    "    #will consider \"correct\" if mean >=0.8, otherwise \"incorrect\" (also a crude metric)\n",
    "    #0.8 value is designed for text-embedding-ada-002 model. If using text-embedding-3-large model, 0.4 works better\n",
    "    return np.mean(np.array(result_1)) \n",
    "\n",
    "\"\"\"\n",
    "Demonstration of how eval_agent and embedding_answers are used to perform the Embed_AI performance evaluation algorithm\n",
    "\n",
    "eval_ai = await eval_agent(this_question, this_answer, this_ideal, eval_model)\n",
    "embedding_eval = await embedding_answers(this_answer, this_ideal, custom_stopwords, english_words)\n",
    "\n",
    "if (embedding_eval >= 0.8 and (eval_ai in [\"Same\", \"Similar\"])):\n",
    "            #Embed_AI evaluation algorithm will consider a generated answer \"correct\" if both the embedding_eval score is >=0.8 and if the AI evaluation returns \"Same\" or \"Similar\".\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmbagent_env",
   "language": "python",
   "name": "cmbagent_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
