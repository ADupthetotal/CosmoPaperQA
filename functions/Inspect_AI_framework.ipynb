{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169e73d-8d69-4e77-a437-165a77fe40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, conlist\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import re\n",
    "from rake_nltk import Rake\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "import nltk\n",
    "import json\n",
    "from typing import Literal\n",
    "from typing import Any\n",
    "\n",
    "from inspect_ai.solver import (\n",
    "    TaskState,\n",
    "    solver,\n",
    ")\n",
    "from inspect_ai import Task, task\n",
    "from inspect_ai.dataset import Sample\n",
    "from inspect_ai.scorer import (\n",
    "    CORRECT,\n",
    "    INCORRECT,\n",
    "    Score,\n",
    "    Target,\n",
    "    accuracy,\n",
    "    stderr,\n",
    "    scorer,\n",
    ")\n",
    "from inspect_ai.solver import bridge\n",
    "from inspect_ai import eval\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#poor man's pass by ref\n",
    "class CSVHolder:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "client = OpenAI(api_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "#Vector store for files used for RAG, change this to whatever vector store you have for OpenAI. \n",
    "#See Create_Vector_Store_Example.ipynb in the repository to see how to do this.\n",
    "vector_store=client.vector_stores.retrieve(vector_store_id=\"vs_67da9f09a6b48191a32189befe73c49e\")\n",
    "\n",
    "#file path to the csv file containing the evaluation dataset\n",
    "lit = pd.read_csv('../cmbagent_dataset/cmbagent_dataset.csv', delimiter=\"\\t\")\n",
    "\n",
    "question=[]\n",
    "ideal=[]\n",
    "for i in range(lit.shape[0]):\n",
    "    question.append(lit.loc[i, \"question\"])\n",
    "    ideal.append(lit.loc[i, \"ideal\"])\n",
    "\n",
    "def inspect_ai_eval(rag_agent, eval_agent, embedding_answers):\n",
    "    \"\"\"\n",
    "    Function to implement inspect_ai evaluation using rag_agent and eval_agent functions\n",
    "\n",
    "    Args:\n",
    "        rag_agent: function that implements the RAG for the generated answers to the questions in the dataset\n",
    "        eval_agent: function that implements the AI evaluation agent for Embed_AI\n",
    "        embedding_answers: function that implements the cosine similarity of vector embeddings component of Embed_AI\n",
    "    Returns:\n",
    "        nominally None, but will print out a csv file with all of the relevant evaluation and RAG information present.\n",
    "    \"\"\"\n",
    "    #setup mytasks for evaluation\n",
    "    mytasks = []\n",
    "    for i in range(len(question)):\n",
    "        mytasks.append({\n",
    "            \"input\": question[i],\n",
    "            \"target\": ideal[i]\n",
    "        })\n",
    "    #setup output DataFrame\n",
    "    new_output_holder= CSVHolder(pd.DataFrame({\n",
    "        'question': pd.Series(dtype='object'),\n",
    "        'answer': pd.Series(dtype='object'),\n",
    "        'ideal': pd.Series(dtype='object'),\n",
    "        'AI_eval': pd.Series(dtype='object'),\n",
    "        'embedding_eval': pd.Series(dtype='float'),\n",
    "        'evaluation': pd.Series(dtype='object')\n",
    "    }))\n",
    "    \n",
    "    async def my_agent(task_input: list[dict[str, Any]]) -> str:\n",
    "        #replace rag_agent function if needed, to implement custom RAG agent.\n",
    "        #Can put OpenAI RAG agent or PaperQA2 RAG agent here\n",
    "        answer=rag_agent(task_input[0][\"content\"], vector_store, \"gpt-4o-mini\")\n",
    "        return answer\n",
    "    \n",
    "    @solver\n",
    "    def my_solver():\n",
    "        async def solve(state: TaskState) -> TaskState:\n",
    "            result = await my_agent(state[\"input\"])\n",
    "            return {\"output\":result}\n",
    "        return solve\n",
    "    \n",
    "    @task\n",
    "    def my_task(tasks):\n",
    "        return Task(\n",
    "            dataset=[Sample(\n",
    "                input=tasks[i][\"input\"],\n",
    "                target=tasks[i][\"target\"]\n",
    "            ) for i in range(len(tasks))],\n",
    "            solver = bridge(my_solver()),\n",
    "            #replace \"gpt-4o-mini\" with model you want to use for evaluation AI\n",
    "            scorer = my_scorer(\"gpt-4o-mini\", custom_stopwords, english_words, new_output_holder),\n",
    "        )\n",
    "    \n",
    "    @scorer(metrics=[accuracy(), stderr()])\n",
    "    def my_scorer(eval_model: str, custom_stopwords: set, english_words: set, new_output_holder):\n",
    "        async def score(state: TaskState, target: Target) -> Score:\n",
    "            this_question = state.input_text\n",
    "            this_answer = state.output.completion\n",
    "            this_ideal = target.text\n",
    "\n",
    "            #replace eval_agent function if needed, to implement custom evaluation agent\n",
    "            eval_ai = await eval_agent(this_question, this_answer, this_ideal, eval_model\n",
    "            embedding_eval = await embedding_answers(this_answer, this_ideal, custom_stopwords, english_words)\n",
    "    \n",
    "            #sort out new_output pass by ref\n",
    "            if (embedding_eval >= 0.8 and (eval_ai in [\"Same\", \"Similar\"])):\n",
    "                #Embed_AI evaluation algorithm will consider a generated answer \"correct\" if both the embedding_eval score is >=0.8 and if the AI evaluation returns \"Same\" or \"Similar\".\n",
    "                new_entry=pd.DataFrame({\"question\":this_question, \"answer\":this_answer, \"ideal\":this_ideal, \"AI_eval\": eval_ai, \"embedding_eval\": embedding_eval, \"evaluation\":\"CORRECT\"}, index=[0])\n",
    "                new_output_holder.value=pd.concat([new_output_holder.value, new_entry], ignore_index=True)\n",
    "                new_output_holder.value.to_csv(\"output_CosmoPaperQA_OpenAI_eval.csv\", index=False)\n",
    "                return Score(value=CORRECT)\n",
    "            else:\n",
    "                new_entry=pd.DataFrame({\"question\":this_question, \"answer\":this_answer, \"ideal\":this_ideal, \"AI_eval\": eval_ai, \"embedding_eval\": embedding_eval, \"evaluation\":\"INCORRECT\"}, index=[0])\n",
    "                new_output_holder.value=pd.concat([new_output_holder.value, new_entry], ignore_index=True)\n",
    "                new_output_holder.value.to_csv(\"output_CosmoPaperQA_OpenAI_eval.csv\", index=False)\n",
    "                return Score(value=INCORRECT)\n",
    "        return score\n",
    "    \n",
    "    new_output_holder.value.to_csv(\"output_CosmoPaperQA_OpenAI_eval.csv\", index=False)\n",
    "    logs = eval(\n",
    "        my_task(mytasks)\n",
    "    )\n",
    "    print(logs)\n",
    "    for log in logs:\n",
    "        print(log.results)\n",
    "    return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmbagent_env",
   "language": "python",
   "name": "cmbagent_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
